{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def find_orb_features(image):\n",
    "  # Convert image to grayscale\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Initiate ORB detector\n",
    "  orb = cv2.ORB_create()\n",
    "\n",
    "  # Find keypoints and descriptors\n",
    "  kp, descriptors = orb.detectAndCompute(gray, None)\n",
    "\n",
    "  return kp, descriptors\n",
    "\n",
    "def match_features(descriptors1, descriptors2, ratio=0.8):\n",
    "  # Create Brute Force Matcher object\n",
    "  matcher = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "\n",
    "  # Perform matching\n",
    "  matches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "  # Filter good matches based on Lowe's ratio test\n",
    "  good_matches = []\n",
    "  for m, n in matches:\n",
    "    if m.distance < ratio * n.distance:\n",
    "      good_matches.append(m)\n",
    "\n",
    "  return good_matches\n",
    "\n",
    "def get_homography(kp1, kp2, matches):\n",
    "  # Convert keypoints to numpy arrays\n",
    "  src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "  dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "  # Find homography using RANSAC\n",
    "  M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "  return M\n",
    "\n",
    "def warp_perspective(img, M):\n",
    "  # Get image height and width\n",
    "  h, w = img.shape[:2]\n",
    "\n",
    "  # Find corners of the image in the destination image\n",
    "  pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "  dst = cv2.perspectiveTransform(pts, M)\n",
    "\n",
    "  # Find minimum and maximum points\n",
    "  x_min = np.min(dst[:, 0, 0])\n",
    "  y_min = np.min(dst[:, 0, 1])\n",
    "  x_max = np.max(dst[:, 0, 0])\n",
    "  y_max = np.max(dst[:, 0, 1])\n",
    "\n",
    "  # Get translation offset\n",
    "  offset_x = int(-x_min)\n",
    "  offset_y = int(-y_min)\n",
    "\n",
    "  # Create new destination image with proper size\n",
    "  h_new = int(y_max - y_min)\n",
    "  w_new = int(x_max - x_min)\n",
    "  dst_img = cv2.warpPerspective(img, M, (w_new + offset_x, h_new + offset_y))\n",
    "\n",
    "  return dst_img, offset_x, offset_y\n",
    "\n",
    "def blend_images(img1, img2, offset_x, offset_y):\n",
    "  # Get image heights and widths\n",
    "  h1, w1 = img1.shape[:2]\n",
    "  h2, w2 = img2.shape[:2]\n",
    "\n",
    "  # Create mask for blending (black for image1, white for image2)\n",
    "  mask = np.zeros((h_new, w_new), np.float32)\n",
    "  mask[offset_y:offset_y+h1, offset_x:offset_x+w1] = 1.0\n",
    "\n",
    "  # Blend images using weighted average\n",
    "  blended_img = (img1.astype(np.float32) * (1 - mask) + img2.astype(np.float32) * mask)\n",
    "\n",
    "  return blended_img.astype(np.uint8)\n",
    "\n",
    "def stitch_images(images):\n",
    "  # Reference image for stitching\n",
    "  reference_img = images[0]\n",
    "\n",
    "  # Resulting panorama image\n",
    "  panorama = None\n",
    "\n",
    "  for i in range(1, len(images)):\n",
    "    # Find ORB features and descriptors\n",
    "    img1_kp, img1_des = find_orb_features(reference_img)\n",
    "    img2_kp, img2_des = find_orb_features(images[i])\n",
    "\n",
    "    # Match features\n",
    "    matches = match_features(img1_des, img2_des)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
